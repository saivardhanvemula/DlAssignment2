# -*- coding: utf-8 -*-
"""DlAssignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QTa-dvrUxJS336yUol6IC-e5REiQADtH
"""

!pip install -q kaggle transformers datasets

import kagglehub
import tensorflow as tf
from transformers import GPT2TokenizerFast, TFGPT2LMHeadModel, create_optimizer
import time
import numpy as np

# # Upload your kaggle.json file here (if using Google Colab)
# from google.colab import files
# files.upload()  # Upload the kaggle.json file here

# # Set the environment variable for Kaggle authentication
# import os
# os.environ['KAGGLE_CONFIG_DIR'] = '/content'  # If uploaded to /content in Colab

import kagglehub

# Download latest version
path = kagglehub.dataset_download("paultimothymooney/poetry")

print("Path to dataset files:", path)

# Load the lyrics file for the chosen artist (example: Michael Jackson)
file_path = f"{path}/michael-jackson.txt"  # Update to your file location

def load_file():
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

text = load_file()

# Initialize the tokenizer
model_id = "gpt2-medium"
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)

# Define constants
MAX_LENGTH = 256
BATCH_SIZE = 2

# Tokenize the entire corpus
tokenized_data = tokenizer(
    text,
    truncation=True,
    max_length=MAX_LENGTH,
    return_overflowing_tokens=True,
    return_length=True
)

# Check tokenized data for debugging
print(tokenized_data)

# Chunk the input text to avoid exceeding memory
text_chunks = [text[i:i + 1000] for i in range(0, len(text), 1000)]
def preprocess(example):
    try:
        outputs = tokenizer(
            example,
            truncation=True,
            max_length=MAX_LENGTH,
            return_overflowing_tokens=True,
            return_length=True,
        )

        input_batch = []
        for input_ids, length in zip(outputs["input_ids"], outputs["length"]):
            if length == MAX_LENGTH:
                input_batch.append(input_ids)

        if not input_batch:
            return {"input_ids": [], "labels": []}

        # Pad if batch is smaller than BATCH_SIZE
        while len(input_batch) < BATCH_SIZE:
            input_batch.append(input_batch[-1])  # pad with last valid sequence

        input_batch = input_batch[:BATCH_SIZE]  # ensure correct batch size

    except Exception as e:
        return {"input_ids": [], "labels": []}

    return {
        "input_ids": input_batch,
        "labels": input_batch  # language modeling: input_ids == labels
    }
def gen():
    for chunk in text_chunks:  # you must chunk long text into small parts
        yield preprocess(chunk)

tf_train_dataset = tf.data.Dataset.from_generator(
    gen,
    output_signature={
        "input_ids": tf.TensorSpec(shape=(BATCH_SIZE, MAX_LENGTH), dtype=tf.int32),
        "labels": tf.TensorSpec(shape=(BATCH_SIZE, MAX_LENGTH), dtype=tf.int32),
    }
)

def gen():
    for chunk in text_chunks:
        processed = preprocess(chunk)
        if processed["input_ids"]:
            yield {
                "input_ids": tf.constant(processed["input_ids"], dtype=tf.int32),
                "labels": tf.constant(processed["labels"], dtype=tf.int32)
            }

# Build TensorFlow dataset
tf_train_dataset = tf.data.Dataset.from_generator(
    gen,
    output_signature={
        "input_ids": tf.TensorSpec(shape=(BATCH_SIZE, MAX_LENGTH), dtype=tf.int32),
        "labels": tf.TensorSpec(shape=(BATCH_SIZE, MAX_LENGTH), dtype=tf.int32),
    }
)

# Calculate training steps
num_train_steps = len(text_chunks) * BATCH_SIZE
print(f"Number of training steps: {num_train_steps}")

# Load model
model = TFGPT2LMHeadModel.from_pretrained(model_id)

# Create optimizer
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1000,
    num_train_steps=num_train_steps
)

# Load the GPT-2 model
# model = TFGPT2LMHeadModel.from_pretrained(model_id)

# # Create optimizer and schedule
# optimizer, schedule = create_optimizer(
#     init_lr=5e-5,
#     num_warmup_steps=1_000,
#     num_train_steps=num_train_steps
# )

# Compile the model
model.compile(optimizer=optimizer)

# Train the model
model.fit(tf_train_dataset, epochs=3)

model.save_pretrained("./fine_tuned_gpt2_poetry")
tokenizer.save_pretrained("./fine_tuned_gpt2_poetry")

# Generate text
input_text = "true love shouldn't be this complicated"
input_ids = tokenizer(input_text, return_tensors="tf")["input_ids"]

start_time = time.time()
output_temp = model.generate(
    input_ids,
    max_length=256,
    do_sample=True,
    temperature=1.0,
    top_k=50
)
print(tokenizer.decode(output_temp[0], skip_special_tokens=True))
print(f"Generation Time: {time.time() - start_time:.2f} seconds")